{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAgodaReview(param):\n",
    "    rs = rq.post(r'https://www.agoda.com/NewSite/en-us/Review/ReviewComments', json = param)\n",
    "    soup = BeautifulSoup(rs.content, 'html.parser')\n",
    "    comments = soup.find_all(\"div\", attrs={\"data-selenium\":\"individual-review-section\"})\n",
    "\n",
    "    result = []\n",
    "    for comment in comments:\n",
    "        title = comment.find(attrs={\"data-selenium\":\"comments-title\"}).text\n",
    "        rate = comment.find(attrs={\"data-selenium\":\"individual-review-rate\"}).text\n",
    "        review = comment.find(attrs={\"data-selenium\":\"reviews-comments\"})\n",
    "        if review is not None:\n",
    "            review = review.text.strip()\n",
    "        result.append([title,rate,review])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createparam(hotelId,pageNum):\n",
    "    param = {\"hotelId\":hotelId,\"providerId\":332,\"demographicId\":0,\"page\":pageNum,\"pageSize\":100,\"sorting\":1,\"providerIds\":332,\n",
    "             \"isReviewPage\":\"false\",\"isCrawlablePage\":\"true\",\"filters\":{\"language\":[23],\"room\":[]},\"searchKeyword\":\"\"}\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getReviews(hotelId):\n",
    "    currentPage = 1\n",
    "    result = []\n",
    "    while True:\n",
    "        param = createparam(hotelId, currentPage)\n",
    "        curr_req = getAgodaReview(param)\n",
    "        result += curr_req\n",
    "        currentPage += 1\n",
    "        if len(curr_req) < 100:\n",
    "            break\n",
    "    \n",
    "    df = pd.DataFrame(result, columns=['title','rating', 'reviews'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_reviews = pd.read_excel(\"D:\\\\Data Science\\\\Hotel-Reviews-BM\\\\all-reviews.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_reviews['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30,10))\n",
    "df_all_reviews['rating'].value_counts().sort_index().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mediocre_rating = df_all_reviews[df_all_reviews['rating'] <= 5.0]\n",
    "df_mediocre_rating.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mediocre_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reviews = df_all_reviews['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex as rgx\n",
    "def replaceShortForm(txt):\n",
    "    txt = rgx.sub(r'([\\.]+)|(\\r\\n)', ' ', txt.lower())\n",
    "    txt = rgx.sub(r'\\s(x)\\s', ' tak ', txt)\n",
    "    txt = rgx.sub(r'\\sx([a-z]+)\\s', r' tak \\1 ', txt)\n",
    "    return txt\n",
    "\n",
    "df_reviews = df_reviews.apply(replaceShortForm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(set(word_tokenize(df_reviews[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = df_reviews.apply(lambda txt: pd.value_counts(word_tokenize(txt))) \\\n",
    "                       .sum(axis=0) \\\n",
    "                       .sort_values(axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword2 = pd.read_csv(r'D:\\Repositories\\stopword.csv', index_col =0 )\n",
    "stopword2 = list(stopword2['Stop_Word'].values)\n",
    "stopword2 += [',','!','?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count_wo_stopword = word_count.drop(axis=0, labels=stopword2, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_count_wo_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "word_count_wo_stopword.head(10).plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modeling of the Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_all_reviews = pd.read_excel(\"D:\\\\Data Science\\\\Hotel-Reviews-BM\\\\all-reviews-sent-only.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3086 entries, 0 to 3085\n",
      "Data columns (total 2 columns):\n",
      "Sentence    3086 non-null object\n",
      "Polarity    1997 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 48.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_all_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_Polarity = df_all_reviews.dropna(axis=0, subset=['Polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1997 entries, 0 to 1998\n",
      "Data columns (total 2 columns):\n",
      "Sentence    1997 non-null object\n",
      "Polarity    1997 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 46.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_with_Polarity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sy tidak gembira, sy menginap di hotel 1 mlm u...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sy terpaksa tanggung kos repair akibat bonet k...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ini lah kali pertama, sy menginap di hotel yg ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sy harap pihak agoda, tidak bekerjasama dgn ho...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oklah</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Polarity\n",
       "0  Sy tidak gembira, sy menginap di hotel 1 mlm u...  Negative\n",
       "1  Sy terpaksa tanggung kos repair akibat bonet k...  Negative\n",
       "2  ini lah kali pertama, sy menginap di hotel yg ...  Negative\n",
       "3  sy harap pihak agoda, tidak bekerjasama dgn ho...  Negative\n",
       "4                                              oklah   Neutral"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_Polarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_Polarity.to_excel(\"D:\\\\Data Science\\\\Hotel-Reviews-BM\\\\sent-polarity.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "converted = pd.read_excel(r'D:\\Data Science\\word_list_airasia_converted.xls', sheet_name='Updated')\n",
    "d = {}\n",
    "for row in converted.itertuples():\n",
    "    d[row[1]] = row[2]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'&': 'dan',\n",
       " 'abgkat': 'abang dekat',\n",
       " 'abis': 'habis',\n",
       " 'ade': 'ada',\n",
       " 'adoi': 'aduh',\n",
       " 'adoiiii': 'aduh',\n",
       " 'aerodarat': 'kapal darat',\n",
       " 'agkt': 'angkat',\n",
       " 'ahh': 'ah',\n",
       " 'ailior': 'air liur',\n",
       " 'airpot': 'lapangan terbang',\n",
       " 'aje': 'sahaja',\n",
       " 'ajelah': 'sahajalah',\n",
       " 'ajer': 'sahaja',\n",
       " 'ak': 'aku',\n",
       " 'ambik': 'ambil',\n",
       " 'amek': 'ambil',\n",
       " 'amekla': 'ambil lah',\n",
       " 'ana': 'saya',\n",
       " 'angkt': 'angkat',\n",
       " 'anual': 'tahunan',\n",
       " 'apa2': 'apa-apa',\n",
       " 'apapun': 'apa pun',\n",
       " 'ape': 'apa',\n",
       " 'arab': 'Arab',\n",
       " 'area': 'kawasan',\n",
       " 'aritu': 'hari itu',\n",
       " 'ask': 'tanya',\n",
       " 'awal2': 'awal-awal',\n",
       " 'babi': 'khinzir',\n",
       " 'badan2': 'badan-badan',\n",
       " 'bag': 'beg',\n",
       " 'baik2': 'baik-baik',\n",
       " 'baling2': 'baling-baling',\n",
       " 'bang': 'abang',\n",
       " 'bangla': 'bangladesh',\n",
       " 'banyk': 'banyak',\n",
       " 'barang2': 'barang-barang',\n",
       " 'bard': 'pujangga',\n",
       " 'bargasi': 'bagasi',\n",
       " 'basikal2': 'basikal-basikal',\n",
       " 'bawak': 'bawa',\n",
       " 'bawanges': 'bawang',\n",
       " 'bdk2': 'budak-budak',\n",
       " 'bebawang': 'bawang-bawang',\n",
       " 'belagak': 'berlagak',\n",
       " 'berat2': 'berat-berat',\n",
       " 'berebut2': 'berebut-rebut',\n",
       " 'berenti': 'berhenti',\n",
       " 'beribu2': 'beribu-ribu',\n",
       " 'berjuta2': 'berjuta-juta',\n",
       " 'berkilo2': 'berkilo-kilo',\n",
       " 'beskal': 'basikal',\n",
       " 'bff': 'rakan karib',\n",
       " 'bg': 'bagi',\n",
       " 'bgi': 'bagi',\n",
       " 'biase': 'biasa',\n",
       " 'bile': 'bila',\n",
       " 'binawe': 'binatang',\n",
       " 'bini': 'isteri',\n",
       " 'bising2': 'bising-bising',\n",
       " 'bkn': 'bukan',\n",
       " 'bla': 'bila',\n",
       " 'blh': 'boleh',\n",
       " 'blom': 'belum',\n",
       " 'bnyak': 'banyak',\n",
       " 'bnyk': 'banyak',\n",
       " \"bo's\": 'bos',\n",
       " 'bole': 'boleh',\n",
       " 'bpe': 'berapa',\n",
       " 'brg': 'barang',\n",
       " 'brg2': 'barang-barang',\n",
       " 'briefing': 'taklimat',\n",
       " 'brng': 'barang',\n",
       " 'bro': 'abang',\n",
       " 'bru': 'baru',\n",
       " 'bruntung': 'beruntung',\n",
       " 'bsikal': 'basikal',\n",
       " 'btnggjwb': 'bertanggungjawab',\n",
       " 'btul2': 'betul-betul',\n",
       " 'buang2': 'buang-buang',\n",
       " 'buatlh': 'buatlah',\n",
       " 'budak²': 'budak-budak',\n",
       " 'buh': 'letak',\n",
       " 'bwk': 'bawa',\n",
       " 'byr': 'bayar',\n",
       " 'bz': 'sibuk',\n",
       " 'camni': 'macam ini',\n",
       " 'campak2': 'campak-campak',\n",
       " 'cane': 'macam mana',\n",
       " 'carakerja': 'cara kerja',\n",
       " 'celako': 'celaka',\n",
       " 'cer': 'cerita',\n",
       " 'ciput': 'sedikit',\n",
       " 'cite': 'cerita',\n",
       " 'citer': 'cerita',\n",
       " 'ckit': 'sikit',\n",
       " 'ckp': 'cakap',\n",
       " 'cm': 'macam',\n",
       " 'cmpak': 'campak',\n",
       " 'cmpk2': 'campak-campak',\n",
       " 'cpt2': 'cepat-cepat',\n",
       " 'cpt²': 'cepat-cepat',\n",
       " 'cr': 'cari',\n",
       " 'cube': 'cuba',\n",
       " 'curinyaaaa': 'curinya',\n",
       " 'cust': 'pelanggan',\n",
       " 'd': 'di',\n",
       " 'da': 'dah',\n",
       " 'dahh': 'dah',\n",
       " 'dapek': 'dapat',\n",
       " 'day': 'hari',\n",
       " 'dbalingnya': 'dibalingnya',\n",
       " 'de': 'ada',\n",
       " 'depa': 'mereka',\n",
       " 'dessa': 'desa',\n",
       " 'dgn': 'dengan',\n",
       " 'dh': 'dah',\n",
       " 'didunia': 'di dunia',\n",
       " 'diorang': 'mereka',\n",
       " 'diorng': 'mereka',\n",
       " 'dkt': 'dekat',\n",
       " 'dlempar2': 'dilempar-lempar',\n",
       " 'dlm': 'dalam',\n",
       " 'dlt': 'padam',\n",
       " 'dlu': 'dulu',\n",
       " 'dorg': 'mereka',\n",
       " 'dpermudhkn': 'dipermudahkan',\n",
       " 'dpt': 'dapat',\n",
       " 'dr': 'dari',\n",
       " 'dri': 'dari',\n",
       " 'dsb': 'dan sebagainya',\n",
       " 'dy': 'dia',\n",
       " 'elok2': 'elok-elok',\n",
       " 'ewah': 'wah',\n",
       " 'fkr': 'fikir',\n",
       " 'free': 'percuma',\n",
       " 'fyi': 'untuk pengetahuan anda',\n",
       " 'gantila': 'gantilah',\n",
       " 'gantirugi': 'ganti rugi',\n",
       " 'gara2': 'gara-gara',\n",
       " 'gerenti': 'jaminan',\n",
       " 'gile': 'gila',\n",
       " 'gk': 'juga',\n",
       " 'gnti': 'ganti',\n",
       " 'gomen': 'kerajaan',\n",
       " 'goment': 'kerajaan',\n",
       " 'guarno': 'macam mana',\n",
       " 'hahaha': 'haha',\n",
       " 'hahahaha': 'haha',\n",
       " 'hahahahaha': 'haha',\n",
       " 'hahhaha': 'haha',\n",
       " 'hampa': 'mereka',\n",
       " 'hampeh': 'teruk',\n",
       " 'hanat2': 'laknat',\n",
       " 'hanta': 'hantar',\n",
       " 'haritu': 'hari itu',\n",
       " 'hawau': 'celaka',\n",
       " 'hempas2': 'hempas-hempas',\n",
       " 'henpon': 'telefon',\n",
       " 'heran': 'hairan',\n",
       " 'hmpa': 'mereka',\n",
       " 'hntr': 'hantar',\n",
       " 'hotak': 'otak',\n",
       " 'hr': 'hari',\n",
       " 'hrga': 'harga',\n",
       " 'hrp': 'harap',\n",
       " 'hu': 'huhu',\n",
       " 'i': 'saya',\n",
       " 'ichi': 'inci',\n",
       " 'idung': 'hidung',\n",
       " 'ig': 'Instagram',\n",
       " 'iklas': 'ikhlas',\n",
       " 'isyaallah': 'insyAllah',\n",
       " 'ja': 'sahaja',\n",
       " 'jd': 'jadi',\n",
       " 'je': 'saja',\n",
       " 'jee': 'saja',\n",
       " 'jek': 'saja',\n",
       " 'jer': 'saja',\n",
       " 'jerr': 'saja',\n",
       " 'jez': 'saja',\n",
       " 'jg': 'jaga',\n",
       " 'jgk': 'juga',\n",
       " 'jgn': 'jangan',\n",
       " 'jgnla': 'janganlah',\n",
       " 'jibake': 'celaka',\n",
       " 'jjur': 'jujur',\n",
       " 'job': 'kerja',\n",
       " 'jogja': 'Jogjakarta',\n",
       " 'jth': 'jatuh',\n",
       " 'jugak': 'juga',\n",
       " 'ka': 'ke',\n",
       " 'kalo': 'kalau',\n",
       " 'kalu': 'kalau',\n",
       " 'kang': 'nanti',\n",
       " 'kantoi': 'kantoi',\n",
       " 'kasi': 'beri',\n",
       " 'kat': 'dekat',\n",
       " 'kbye': 'ok bye',\n",
       " 'kearah': 'ke arah',\n",
       " 'kecik': 'kecil',\n",
       " 'keja': 'kerja',\n",
       " 'keje': 'kerja',\n",
       " 'kejo': 'kerja',\n",
       " 'kekatak': 'katak-katak',\n",
       " 'keksongan': 'kekosongan',\n",
       " 'kemana': 'ke mana',\n",
       " 'kene': 'kena',\n",
       " 'kenekan': 'kenakan',\n",
       " 'kesah': 'kisah',\n",
       " 'ketempat': 'ke tempat',\n",
       " 'kg': 'kampung',\n",
       " 'kije': 'kerja',\n",
       " 'kijo': 'kerja',\n",
       " 'kite': 'kita',\n",
       " 'kito': 'kita',\n",
       " 'kje': 'kerja',\n",
       " 'kjr': 'kerja',\n",
       " 'kl': 'Kuala Lumpur',\n",
       " 'klau': 'kalau',\n",
       " 'klia2': 'KLIA 2',\n",
       " 'klo': 'kalau',\n",
       " 'klu': 'kalau',\n",
       " 'kn': 'kan',\n",
       " 'knapa': 'kenapa',\n",
       " 'kne': 'kena',\n",
       " 'ko': 'kau',\n",
       " 'komen2': 'komen-komen',\n",
       " 'kompom': 'sah',\n",
       " 'korang': 'kamu semua',\n",
       " 'korg': 'kamu semua',\n",
       " 'kot': 'mungkin',\n",
       " 'krja': 'kerja',\n",
       " 'ksalahan': 'kesalahan',\n",
       " 'kt': 'dekat',\n",
       " 'kta': 'kita',\n",
       " 'kuar': 'keluar',\n",
       " 'kut': 'mungkin',\n",
       " 'la': 'lah',\n",
       " 'laa': 'lah',\n",
       " 'lahabau': 'celaka',\n",
       " 'lahanat': 'celaka',\n",
       " 'lainda': 'lain dah',\n",
       " 'lak': 'pula',\n",
       " 'lama2': 'lama-lama',\n",
       " 'last2': 'akhir sekali',\n",
       " 'lawak2': 'lawak-lawak',\n",
       " 'le': 'lah',\n",
       " 'ler': 'lah',\n",
       " 'letak2': 'letak-letak',\n",
       " 'lg': 'lagi',\n",
       " 'lgi': 'lagi',\n",
       " 'lngsong': 'langsung',\n",
       " 'lol': 'haha',\n",
       " 'lorrr': 'lah',\n",
       " 'lps': 'lepas',\n",
       " 'lumbe': 'lumba',\n",
       " 'lyak': 'layak',\n",
       " 'maaf2': 'maaf-maaf',\n",
       " 'maap': 'maaf',\n",
       " 'maapkan': 'maafkan',\n",
       " 'mahai': 'mahal',\n",
       " 'mahal2': 'mahal-mahal',\n",
       " 'main2': 'main-main',\n",
       " 'mampos': 'mampus',\n",
       " 'mau': 'mahu',\n",
       " 'mcm': 'macam',\n",
       " 'mcmtu': 'macam itu',\n",
       " 'memerlukn': 'memerlukan',\n",
       " 'mengembirakan': 'menggembirakan',\n",
       " 'mengmbilnyer': 'mengambilnya',\n",
       " 'mengtasi': 'mengatasi',\n",
       " 'mentang2': 'mentang-mentang',\n",
       " 'mg': 'memang',\n",
       " 'mgkn': 'mungkin',\n",
       " 'mihak': 'memihak',\n",
       " 'mingu': 'minggu',\n",
       " 'mintak': 'minta',\n",
       " 'mjtuhkn': 'menjatuhkan',\n",
       " 'mkyong': 'mak yong',\n",
       " 'mlibatkn': 'melibatkan',\n",
       " 'mlm': 'malam',\n",
       " 'mmg': 'memang',\n",
       " 'mmnjang': 'memanjang',\n",
       " 'mmpos': 'mampus',\n",
       " 'mn': 'mana',\n",
       " 'mna': 'mana',\n",
       " 'mntak': 'minta',\n",
       " 'mntk': 'minta',\n",
       " 'mnyusun': 'menyusun',\n",
       " 'msa': 'masa',\n",
       " 'msia': 'Malaysia',\n",
       " 'mst': 'mesti',\n",
       " 'mu': 'awak',\n",
       " 'much': 'banyak',\n",
       " 'muko': 'muka',\n",
       " 'n': 'dan',\n",
       " 'nah': 'Nah',\n",
       " 'nanges': 'menangis',\n",
       " 'nangis': 'menangis',\n",
       " 'napo': 'kenapa',\n",
       " 'nati': 'nanti',\n",
       " 'ngan': 'dengan',\n",
       " 'ngn': 'dengan',\n",
       " 'ni': 'ini',\n",
       " 'nie': 'ini',\n",
       " 'niiiii': 'ini',\n",
       " 'nk': 'nak',\n",
       " 'nmpk': 'nampak ',\n",
       " 'nye': 'nya',\n",
       " 'ofis': 'pejabat',\n",
       " 'ohh': 'oh',\n",
       " 'oii': 'hoi',\n",
       " 'oiiiii': 'hoi',\n",
       " 'org': 'orang',\n",
       " 'orng': 'orang',\n",
       " 'otek': 'otak',\n",
       " 'p': 'pergi',\n",
       " 'pakai2': 'pakai-pakai',\n",
       " 'palabana': 'kepala otak',\n",
       " 'pasni': 'lepas ini',\n",
       " 'pastu': 'lepas itu',\n",
       " 'pd': 'pada',\n",
       " 'pegi': 'pergi',\n",
       " 'pekerja2': 'pekerja-pekerja',\n",
       " 'pekerje': 'pekerja',\n",
       " 'pekrja': 'pekerja',\n",
       " 'perabih': 'perabis',\n",
       " 'perkerja': 'pekerja',\n",
       " 'phuii': 'puih',\n",
       " 'pikir': 'fikir',\n",
       " 'pisanglah': 'pisang',\n",
       " 'pk': 'fikir',\n",
       " 'pkerja': 'pekerja',\n",
       " 'pkerjaan': 'pekerjaan',\n",
       " 'pki': 'pakai',\n",
       " 'please': 'tolong',\n",
       " 'pls': 'tolong',\n",
       " 'pn': 'puan',\n",
       " 'pnh': 'pernah',\n",
       " 'pnt': 'penat',\n",
       " 'pny': 'punya',\n",
       " 'pnya': 'punya',\n",
       " 'pon': 'pun',\n",
       " 'priority': 'keutamaan',\n",
       " 'properties': 'harta benda',\n",
       " 'ptugas': 'petugas',\n",
       " 'puas2': 'puas-puas',\n",
       " 'pub': 'kelab malam',\n",
       " 'pulak': 'pula',\n",
       " 'puye': 'punya',\n",
       " 'pwrcuma': 'percuma',\n",
       " 'pyahnya': 'payahnya',\n",
       " 'rasa2nya': 'rasa-rasanya',\n",
       " 'rege': 'harga',\n",
       " 'reger': 'harga',\n",
       " 'rosok': 'rosak',\n",
       " 'rse': 'rasa',\n",
       " 'sado': 'tegap',\n",
       " 'sam': 'sama',\n",
       " 'samp': 'sampah',\n",
       " 'sb': 'sebab',\n",
       " 'sbb': 'sebab',\n",
       " 'sbgai': 'sebagai',\n",
       " 'sblm': 'sebelum',\n",
       " 'sblum': 'sebelum',\n",
       " 'sbnarnya': 'sebenarnya',\n",
       " 'sbum': 'sebelum',\n",
       " 'sdg': 'sedang',\n",
       " 'sebb': 'sebab',\n",
       " 'sebijik': 'sebiji',\n",
       " 'selfie': 'swafoto',\n",
       " 'sempoi': 'cantik',\n",
       " 'senaraihitam': 'senarai hitam',\n",
       " 'seorg': 'seorang',\n",
       " 'sgt': 'sangat',\n",
       " 'shj': 'sahaja',\n",
       " 'sib': 'nasib',\n",
       " 'skali': 'sekali',\n",
       " 'sket': 'sikit',\n",
       " 'sma2': 'sama-sama',\n",
       " 'smoga': 'semoga',\n",
       " 'smpoi': 'cantik',\n",
       " 'sndiri': 'sendiri',\n",
       " 'sndr': 'sendiri',\n",
       " 'sndri': 'sendiri',\n",
       " 'sne': 'sana',\n",
       " 'sorang': 'seorang',\n",
       " 'sronok': 'seronok',\n",
       " 'ssh': 'susah',\n",
       " 'staf2': 'staf-staf',\n",
       " 'staf2nya': 'staf-stafnya',\n",
       " 'staff': 'staf',\n",
       " 'staffs': 'staf-staf',\n",
       " 'stiap': 'setiap',\n",
       " 'sungguh²': 'sungguh-sungguh',\n",
       " 'sy': 'saya',\n",
       " 'sykt': 'syarikat',\n",
       " 't': 'nanti',\n",
       " 'tah': 'entah',\n",
       " 'taik': 'tahi',\n",
       " 'takan': 'tak akan',\n",
       " 'takat': 'setakat',\n",
       " 'takde': 'tak ada',\n",
       " 'takkan': 'tak akan',\n",
       " 'taknak': 'tak nak',\n",
       " 'tang': 'tentang',\n",
       " 'taraa': 'sementara',\n",
       " 'tau': 'tahu',\n",
       " 'tbabit': 'terbabit',\n",
       " 'terbaekkkk': 'terbaik',\n",
       " 'teruknye': 'teruknya',\n",
       " 'tgh': 'tengah',\n",
       " 'tgk': 'tengok',\n",
       " 'tiap2': 'tiap-tiap',\n",
       " 'tk': 'tak',\n",
       " 'tlg': 'tolong',\n",
       " 'tnggongjwb': 'tanggungjawab',\n",
       " 'tngok': 'tengok',\n",
       " 'tngu': 'tunggu',\n",
       " 'tny': 'tanya',\n",
       " 'tosak': 'rosak',\n",
       " 'tp': 'tapi',\n",
       " 'tpi': 'tapi',\n",
       " 'tpon': 'telefon',\n",
       " 'trgelak': 'tergelak',\n",
       " 'ts': 'Tan Sri',\n",
       " 'tu': 'itu',\n",
       " 'tuh': 'itu',\n",
       " 'tula': 'itulah',\n",
       " 'u': 'awak',\n",
       " 'umeno': 'UMNO',\n",
       " 'undang2': 'undang-undang',\n",
       " 'upkan': 'naikkan',\n",
       " 'ur': 'awak',\n",
       " 'utk': 'untuk',\n",
       " 'viral': 'tular',\n",
       " 'vote': 'undi',\n",
       " 'wassap': 'Whatsapp',\n",
       " 'wat': 'apa',\n",
       " 'weii': 'wei',\n",
       " 'weiii': 'wei',\n",
       " 'woi': 'hey',\n",
       " 'wt': 'buat',\n",
       " 'x': 'tak',\n",
       " 'xakan': 'tak akan',\n",
       " 'xberkat': 'tak berkat',\n",
       " 'xboleh': 'tak boleh',\n",
       " 'xbuat': 'tak buat',\n",
       " 'xdak': 'tak ada',\n",
       " 'xde': 'tak ada',\n",
       " 'xguna': 'tak guna',\n",
       " 'xikhlas': 'tak ikhlas',\n",
       " 'xkn': 'tak akan',\n",
       " 'xluak': 'tak luak',\n",
       " 'xnak': 'tak nak',\n",
       " 'xnmpk': 'tak nampak',\n",
       " 'xpernah': 'tak pernah',\n",
       " 'xpilih': 'tak pilih',\n",
       " 'xsampai': 'tak sampai',\n",
       " 'xsenonoh': 'tak senonoh',\n",
       " 'xsmpat': 'tak sempat',\n",
       " 'xtgk': 'tak tengok',\n",
       " 'xtw': 'tak tahu',\n",
       " 'y': 'yang',\n",
       " 'ye': 'ya',\n",
       " 'yee': 'ya',\n",
       " 'yg': 'yang',\n",
       " 'yng': 'yang',\n",
       " 'you': 'awak',\n",
       " 'your': 'awak'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "word_array = word_tokenize(df_with_Polarity['Sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dan'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['&']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sy',\n",
       " 'tidak',\n",
       " 'gembira',\n",
       " ',',\n",
       " 'sy',\n",
       " 'menginap',\n",
       " 'di',\n",
       " 'hotel',\n",
       " '1',\n",
       " 'mlm',\n",
       " 'utk',\n",
       " '2',\n",
       " 'bilik',\n",
       " ',',\n",
       " 'manager',\n",
       " 'security',\n",
       " 'sombong',\n",
       " 'dan',\n",
       " 'berlagak',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saya',\n",
       " 'tidak',\n",
       " 'gembira',\n",
       " ',',\n",
       " 'saya',\n",
       " 'menginap',\n",
       " 'di',\n",
       " 'hotel',\n",
       " '1',\n",
       " 'malam',\n",
       " 'untuk',\n",
       " '2',\n",
       " 'bilik',\n",
       " ',',\n",
       " 'manager',\n",
       " 'security',\n",
       " 'sombong',\n",
       " 'dan',\n",
       " 'berlagak',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx,word in enumerate(word_array):\n",
    "    if word.isalpha() and word.lower() in d:\n",
    "        word_array[idx] = d[word.lower()]\n",
    "\n",
    "word_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex as rgx\n",
    "class ReplaceWord(object):\n",
    "    def __init__(self):\n",
    "        self.shortform_dict = self.load_shortform_dict()\n",
    "        self.english_dict = self.load_dictionary(r'D:\\Data Science\\Hotel-Reviews-BM\\words_alpha.txt')\n",
    "        self.bm_dict = self.load_dictionary(r'D:\\Data Science\\Hotel-Reviews-BM\\dict-ms-wiki.txt')\n",
    "        \n",
    "        #; kata ganda has problem to detect whether number or words.\n",
    "        self.kata_ganda = rgx.compile(r'(di|ke)?(\\w+)2(kan|nya)?')\n",
    "        self.kata_ganda_replacement = r'\\1\\2-\\2\\3'\n",
    "    \n",
    "    def load_shortform_dict(self):\n",
    "        converted = pd.read_excel(r'D:\\Data Science\\word_list_airasia_converted.xls', sheet_name='Updated')\n",
    "        d = {}\n",
    "        for row in converted.itertuples():\n",
    "            d[row[1]] = row[2]\n",
    "        return d\n",
    "    \n",
    "    def load_dictionary(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as dict_file:\n",
    "            dict_array = dict_file.readlines()\n",
    "        dict_file.close()\n",
    "        return [rgx.sub(r'\\n', '', word) for word in dict_array]\n",
    "    \n",
    "    def replace_multi_char(self, word):\n",
    "        repeat_regexp = rgx.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        repl = r'\\1\\2\\3'\n",
    "        \n",
    "        if word in self.bm_dict or word in self.english_dict:\n",
    "            return word\n",
    "        \n",
    "        repl_word = repeat_regexp.sub(repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace_multi_char(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    " \n",
    "    def check_replacement(self, word_array):\n",
    "        # replace kata ganda\n",
    "        word_array = [self.kata_ganda.sub(self.kata_ganda_replacement, word) for word in word_array]\n",
    "        \n",
    "        # replace words with multiple char\n",
    "        word_array = [self.replace_multi_char(word) for word in word_array]\n",
    "        \n",
    "        # check for shortforms\n",
    "        for idx,word in enumerate(word_array):\n",
    "            is_kata_ganda = False\n",
    "            if \"-\" in word and len(word)>=3:\n",
    "                word = word.split(\"-\")[0]\n",
    "                is_kata_ganda = True\n",
    "\n",
    "            if word.lower() in self.shortform_dict:\n",
    "                converted_word = self.shortform_dict[word.lower()] + \"-\" + self.shortform_dict[word.lower()] if is_kata_ganda else self.shortform_dict[word.lower()] \n",
    "                word_array[idx] = converted_word\n",
    "\n",
    "        # determine if malay words or english\n",
    "        word_not_exist = []\n",
    "        for word in word_array:\n",
    "            if word.lower() not in self.english_dict and word.lower() not in self.bm_dict and word.isalpha():\n",
    "                word_not_exist.append(word)\n",
    "        \n",
    "        return {'result': ' '.join(word_array), 'out_of_list': word_not_exist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "class WordCleaner(object):  \n",
    "    def remove_rep_non_chars(self, txt):\n",
    "        pattern = rgx.compile(r'([\\W_])\\1{2,}')\n",
    "        return pattern.sub(r'\\1', txt)\n",
    "\n",
    "    # pad fullstop with space, since tokenizer cannot detect dot if no space after it\n",
    "    def remove_multi_dots(self, txt):\n",
    "        pattern = rgx.compile(r'(\\w)(\\.{2,})')\n",
    "        return pattern.sub(r'\\1. ', txt)\n",
    "\n",
    "    def pad_dot_with_space(self, txt):\n",
    "        pattern = rgx.compile(r'(\\w)(?:\\s*)\\.(\\w)')\n",
    "        return pattern.sub(r'\\1. \\2', txt)\n",
    "\n",
    "    def remove_new_line(self, txt):\n",
    "        pattern = rgx.compile(r'(\\r|\\n)?')\n",
    "        return pattern.sub('', txt).strip()\n",
    "\n",
    "    def kata_nafi(self, txt):            \n",
    "        kata_nafi = rgx.compile(r'\\b(?:x)(\\w*)\\b')\n",
    "        kata_nafi_replacement = r'tak \\1'\n",
    "        return kata_nafi.sub(kata_nafi_replacement, txt)\n",
    "    \n",
    "    def replace_multi_dots(self, word):\n",
    "        repeat_regexp = rgx.compile(r'(\\w*)(\\.)\\2(\\w*)')\n",
    "        repl = r'\\1\\2\\3'\n",
    "        repl_word = repeat_regexp.sub(repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace_multi_dots(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "    \n",
    "    def tokenize_sentence(self, paragraph_str):\n",
    "        paragraph = self.clean_sentence(paragraph_str)\n",
    "        sentences = sent_tokenize(paragraph)\n",
    "        return sentences\n",
    "    \n",
    "    def clean_sentence(self, sent):\n",
    "        return self.kata_nafi(self.pad_dot_with_space(self.remove_rep_non_chars(self.replace_multi_dots(sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replacer = ReplaceWord()\n",
    "cleaner = WordCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_str2 = 'terlalu tamak.... tu ja nak kata... try bila org bg... 3k sorang... Ambil la... Brg keperluan yg penting, susu anak, pampers,beras ...benda yg wajib setiap bulan beli.... Kan jimat... Kalau dpt kat aku... Beli susu n pampers anak. Jimat dah setahun.....'\n",
    "result = [replacer.check_replacement(word_tokenize(sent_array))[\"result\"] for sent_array in cleaner.tokenize_sentence(sample_str2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['terlalu tamak .',\n",
       " 'itu sahaja nak kata .',\n",
       " 'try bila orang bagi .',\n",
       " '3k seorang .',\n",
       " 'Ambil lah .',\n",
       " 'barang keperluan yang penting , susu anak , pampers , beras .',\n",
       " 'benda yang wajib setiap bulan beli .',\n",
       " 'Kan jimat .',\n",
       " 'Kalau dapat dekat aku .',\n",
       " 'Beli susu dan pampers anak .',\n",
       " 'Jimat dah setahun .']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_sentence(sent):\n",
    "    no_punct_sent = cleaner.clean_sentence(sent)\n",
    "    #print(no_punct_sent)\n",
    "    replaced_word = replacer.check_replacement(word_tokenize(no_punct_sent))['result']\n",
    "    return replaced_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dan lokasi yang sesuai yang mana berhampiran dengan Kuala Lumpur sentral dan jalan tidak berapa sesak di pagi hari'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_sentence('& lokasi yang sesuai yang mana berhampiran dengan kl sentral dan jalan tidak berapa sesak di pagi hari')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clean_sent = df_with_Polarity['Sentence'].apply(normalize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_Polarity = df_with_Polarity.assign(Clean_Sentence = df_clean_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_Polarity = df_with_Polarity[['Polarity', 'Sentence', 'Clean_Sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_Polarity.to_excel(\"D:\\\\Data Science\\\\Hotel-Reviews-BM\\\\clean-polarity-sent.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r'D:\\encycloDB\\Dirty Words\\DirtyWords.json', 'rb') as file:\n",
    "    data = file.read()\n",
    "dirty_words = json.loads(data.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_dirty_words = [ word[\"word\"] for word in dirty_words[\"RECORDS\"] if word[\"language\"] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(en_dirty_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dataset = pd.read_excel(\"D:\\\\Data Science\\\\Hotel-Reviews-BM\\\\clean-polarity-sent.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data_to_model = df_dataset[['Clean_Sentence','Polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df_data_to_model, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=train_data['Clean_Sentence'].values\n",
    "y_train=train_data['Polarity'].values\n",
    "\n",
    "x_test=test_data['Clean_Sentence'].values\n",
    "y_test=test_data['Polarity'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tanya_list = ['kenapa','bila','siapa','mengapa','apa','bagaimana','berapa','mana']\n",
    "perintah_list = ['jangan','sila','tolong','harap','usah','jemput','minta']\n",
    "pangkal_list = ['maka','alkisah','arakian','syahdah','adapun','bermula','kalakian']\n",
    "bantu_list = ['akan','telah','boleh','mesti','belum','sudah','dapat','masih','harus','hendak']\n",
    "penguat_list = ['paling','agak','sungguh','amat','terlalu','nian','benar','paling']\n",
    "penegas_list = ['jua','juga','sahaja','hanya','memang','lagi','pun']\n",
    "nafi_list = ['bukan','tidak']\n",
    "pemeri_list = ['ialah','adalah']\n",
    "sendi_list = ['akan','kepada','terhadap','bagi','untuk','dari','daripada','di','dengan','hingga','sampai',\n",
    "              'ke','kepada','oleh','pada','sejak','seperti','umpama','bak','tentang','laksanabagai',\n",
    "             'semenjak','dalam','antara']\n",
    "pembenar_list = ['ya','benar','betul']\n",
    "nombor_list = ['satu','dua','tiga','empat','lima','enam','tujuh','lapan','sembilan','kosong']\n",
    "suku_bilangan_list = ['per','suku','setengah','separuh','tiga suku']\n",
    "pisahan_list = ['setiap','tiap']\n",
    "keterangan_list = ['begitu','begini','demikian','perlahan','cepat','lena','akan','sedang','belum',\n",
    "                   'telah','sekarang','sebentar','semalam','mungkin','agak','barangkali','pasti','tentu',\n",
    "                   'sudah','selalu','kadang','acapkali','sesekali','yang']\n",
    "arah_list = ['atas','bawah','tepi','antara','hadapan','utara','sisi','luar']\n",
    "hubung_list = ['agar','apabila','atau','bahawa','dan','hingga','jika','jikalau','kecuali','kerana',\n",
    "               'lalu','manakala','sambil','serta','semenjak','sementara','sungguhpun','supaya','walaupun','tetapi']\n",
    "gantinama_list = ['aku','saya','hamba','patik','beta','kami','kita','anda','awak','engkau','tuanku','kalian',\n",
    "                  'kamu','baginda','beliau','mereka']\n",
    "\n",
    "# pos permulaan[:-4]\n",
    "permulaan = ['bel','be','se','ter','men','memper','di','pe','me','ke','ber','pen','per']\n",
    "# pos hujung [:1]\n",
    "hujung = ['kan', 'kah','lah','tah','nya','an','wan','wati','ita']\n",
    "alphabet = 'qwertyuiopasdfghjklzxcvbnm'\n",
    "\n",
    "tatabahasa_dict = {'KT':tanya_list,'KP':perintah_list,'KPA':pangkal_list,'KB':bantu_list,'KPENGUAT':penguat_list,\n",
    "                   'KPENEGAS':penegas_list,'KN':nafi_list, 'KPEMERI':pemeri_list,'KS':sendi_list,'KPEMBENAR':pembenar_list,\n",
    "                   'NO':nombor_list,'SUKU':suku_bilangan_list,'PISAHAN':pisahan_list,'KETERANGAN':keterangan_list,\n",
    "                   'ARAH':arah_list,'KH':hubung_list,'GN':gantinama_list}\n",
    "\n",
    "stopword_tatabahasa = list(set(tanya_list+perintah_list+pangkal_list+bantu_list+penguat_list+\\\n",
    "                penegas_list+nafi_list+pemeri_list+sendi_list+pembenar_list+nombor_list+\\\n",
    "                suku_bilangan_list+pisahan_list+keterangan_list+arah_list+hubung_list+gantinama_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword_tatabahasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r'D:\\Malaya\\malaya\\stop-word-kerulnet','r') as f:\n",
    "    content = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "stopwords = frozenset(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "## Before optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words=stopwords)), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(x_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf.predict(['Saya pasti akan kembali semula ke hotel ini.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), \\\n",
    "              'clf__fit_prior': (True, False), 'clf__alpha':[0.1,0.5,0.8,1.0]}\n",
    "\n",
    "gs_clf_NB = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf_NB = gs_clf_NB.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf_NB.best_score_\n",
    "gs_clf_NB.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running with recommended parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf_cv = Pipeline([('vect', CountVectorizer(stop_words=stopwords, ngram_range=(1, 1))), ('tfidf', TfidfTransformer(use_idf=False)), ('clf', MultinomialNB(alpha=0.5, fit_prior=False))])\n",
    "text_clf_cv = text_clf_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_cv = text_clf_cv.predict(x_test)\n",
    "np.mean(predicted_cv == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predicted_cv, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf_cv.predict(['Bilik yang disediakan agak kecil.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(text_clf_cv, 'sentiment-hotel.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('sentiment-hotel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.predict(['Check in mengambil masa terlalu lama.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "with open(r'D:\\Malaya\\malaya\\stop-word-kerulnet','r') as f:\n",
    "    content = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "stopwords = frozenset(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "stop_words=frozenset(stopwords)\n",
    "text_clf_maxent = Pipeline([('vect', CountVectorizer(stop_words=stop_words, ngram_range=(1, 1))), ('tfidf', TfidfTransformer(use_idf = True)),\n",
    "                         ('clf-svm', LogisticRegression(multi_class='multinomial', solver='saga'))])\n",
    "\n",
    "text_clf_maxent = text_clf_maxent.fit(x_train, y_train)\n",
    "predicted_maxent = text_clf_maxent.predict(x_test)\n",
    "np.mean(predicted_maxent == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), \\\n",
    "              'clf-svm__solver': ['newton-cg', 'sag', 'saga']}\n",
    "\n",
    "gs_clf_maxEnt = GridSearchCV(text_clf_maxent, parameters, n_jobs=-1)\n",
    "gs_clf_maxEnt = gs_clf_maxEnt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf_maxEnt.best_score_\n",
    "gs_clf_maxEnt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predicted_maxent, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words=stop_words)), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=500, \n",
    "                                                   tol=1e-2, random_state=42, n_jobs=-1, learning_rate='invscaling',\n",
    "                                                  eta0=100))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(x_train, y_train)\n",
    "predicted_svm = text_clf_svm.predict(x_test)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TO DO\n",
    "\n",
    "1. Parse time\n",
    "2. dots with no space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
